{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering - 4 Clusters\n",
    "\n",
    "### DataPoint\n",
    "First, we'll have to create a DataPoint class, that can be used to store and compare input-datapoints. We'll do this as follows, including a ```dist``` function to compute the squared-distance between two datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import is required in python version < 3.11 to allos reference to DataPoint inside definition of DataPoint\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def sqr(x: float):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "class DataPoint:\n",
    "    x: float\n",
    "    y: float\n",
    "    a: float\n",
    "    b: float\n",
    "    cluster: int\n",
    "    fidelity: float\n",
    "\n",
    "    def __init__(self, x: float, y: float, a: float, b: float):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.cluster = 0\n",
    "        self.fidelity = 0\n",
    "    \n",
    "    # Minkowski distance metric\n",
    "    def dist(self, o: DataPoint, exp = 5) -> float:\n",
    "        return math.pow(math.pow(abs(o.x - self.x), exp) + math.pow(abs(o.y - self.y), exp) \n",
    "                        + math.pow(abs(o.a - self.a), exp)  + math.pow(abs(o.b - self.b), exp) , 1 / exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also implement the ```map``` function, that can be used to map Data Points onto the Bloch Sphere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(a: float, b: float, c: float, d: float, e: float) -> float:\n",
    "    return d + ((a - b)/(c - b)) * (e - d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qiskit\n",
    "Now, we start working with Qiskit! Import all the necessary elements and the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qiskit.providers.aer import Aer\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector, DensityMatrix\n",
    "from qiskit import transpile\n",
    "from qiskit.algorithms.optimizers import SPSA\n",
    "from qiskit.algorithms.optimizers import ADAM\n",
    "from qiskit.algorithms.optimizers import COBYLA\n",
    "from qiskit.algorithms.optimizers import GradientDescent\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to process the iris dataset. We create a list of DataPoints out of the loaded features and calculate the (Minkowski) distance matrix that stores the distances between each pair of points. We also determine the maximum distance in order to normalize the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hard-code the data points that the algorithm will be performed on\n",
    "#\n",
    "x_range = iris.data[:, 1]\n",
    "y_range = iris.data[:, 3]\n",
    "\n",
    "a_range = iris.data[:, 0]\n",
    "b_range = iris.data[:, 2]\n",
    "\n",
    "data_points = []\n",
    "\n",
    "# Determine minimum and maximum values per feature for normalization\n",
    "min_x = 2\n",
    "max_x = 4.4\n",
    "\n",
    "min_y = 0.1\n",
    "max_y = 2.5\n",
    "\n",
    "min_a = 4.3\n",
    "max_a = 7.9\n",
    "\n",
    "min_b = 1\n",
    "max_b = 6.9\n",
    "\n",
    "for l in range(len(x_range)):\n",
    "    data_points.append(DataPoint(x_range[l], y_range[l], a_range[l], b_range[l]))\n",
    "\n",
    "# Plot the datapoints projected onto the second and fourth feature\n",
    "plt.title(\"Input Data Points\")\n",
    "plt.scatter(\n",
    "    [dp.x for dp in data_points],\n",
    "    [dp.y for dp in data_points]\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the distance matrix of each pair of datapoints\n",
    "max_distance = 0\n",
    "distance_matrix = [[None for j in data_points] for i in data_points]\n",
    "        \n",
    "for i, dp_i in enumerate(data_points):\n",
    "    for j, dp_j in enumerate(data_points):\n",
    "        curr_distance = dp_i.dist(dp_j)\n",
    "        distance_matrix[i][j] = curr_distance\n",
    "        if (curr_distance > max_distance):\n",
    "            max_distance = curr_distance\n",
    "\n",
    "# Normalize the distance matrix\n",
    "for i, dp_i in enumerate(data_points):\n",
    "    for j, dp_j in enumerate(data_points):\n",
    "        distance_matrix[i][j] /= max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function constructs the variational circuit to which we feed our datapoint. The parameters of the initializing universal gates are determined from the data point's features and scaled into range of \\[0, Ï€\\]. In this experiment, we use the EfficientSU2 ansatz with four rotational layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_counter = 0\n",
    "\n",
    "# Function: setup the variational form according to given parameters\n",
    "def setup_var_form(dp: DataPoint, params, qubits = 2) -> QuantumCircuit:\n",
    "    global circuit_counter\n",
    "    \n",
    "    q_circuit = QuantumCircuit(qubits)\n",
    "    q_circuit.name = \"circuit\" + str(circuit_counter)\n",
    "    circuit_counter += 1\n",
    "    \n",
    "    # Initialize both qubits in state zero\n",
    "    q_circuit.initialize([1, 0], 0)\n",
    "    q_circuit.initialize([1, 0], 1)\n",
    "    \n",
    "    # Unitary transformations that insert data-point properties\n",
    "    # for i in range(qubits):\n",
    "    x_map = map(dp.x, min_x, max_x, 0, math.pi)\n",
    "    y_map = map(dp.y, min_y, max_y, 0, math.pi)\n",
    "    q_circuit.u(x_map,\n",
    "                y_map, 0, 0) # datapoints x and y features on qubit 0\n",
    "    \n",
    "    a_map = map(dp.a, min_a, max_a, 0, math.pi)\n",
    "    b_map = map(dp.b, min_b, max_b, 0, math.pi)\n",
    "    q_circuit.u(a_map,\n",
    "                b_map, 0, 1) # datapoints a and b features on qubit 1\n",
    "    \n",
    "    # Use EfficientSU2 variational form with input parameters\n",
    "    ansatz = EfficientSU2(num_qubits=2, su2_gates=['ry', 'rz'], entanglement=\"linear\")\n",
    "    bind_dict = {}\n",
    "    counter = 0\n",
    "    for key in ansatz.parameters:\n",
    "        bind_dict[key] = params[counter]\n",
    "        counter += 1\n",
    "    ansatz = ansatz.bind_parameters(bind_dict)\n",
    "    \n",
    "    q_circuit.compose(ansatz, inplace=True)\n",
    "    q_circuit.save_statevector()  \n",
    "    return q_circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the setup of our algorithm, we need to define the objective function, which is parametrized by the set of optimization parameters. For this experiment, we use the revised cost function from the paper: $$H = {\\sum \\limits_{i,j=1}^N (d(\\vec x_i, \\vec x_j)^\\alpha + \\lambda d(\\vec x_i, \\vec c_i) ) \\sum \\limits_{a=1}^K  (1 - f_i^a) (1 - f_j^a)}$$ where $c_i$ refers to the centroid of the cluster that point i currently belongs to, while $\\alpha$ and $\\lambda$ are regularization parameters. Furthermore, we need to constrain the cost function so that the data points are only clustered into one of the reference states. We can formulate the constraint mathematically: $$\\sum \\limits_{a=1}^K q_i^a  = 1\\ \\  \\ \\forall i.$$ This constraint can be incorporated into our cost function using Lagrange multipliers, obtaining an additional penalty term: $$p_i = \\left(\\sum \\limits_{a=1}^K f_i^a - 1\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def objective(params) -> float:\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    # Select the qiskit backend\n",
    "    qiskit_backend = Aer.get_backend(\"aer_simulator\")\n",
    "    \n",
    "    # Cache computed values to avoid recomputation in nested loops\n",
    "    cache = []\n",
    "    fidelities = [[None for i in range(len(data_points))] for j in range(len(reference_points))]\n",
    "    \n",
    "    # Accumulators for datapoints per cluster\n",
    "    centroids = [DataPoint(0, 0, 0, 0) for i in range(len(reference_points))]\n",
    "    counts = [0 for i in range(len(reference_points))]\n",
    "    \n",
    "    for i in range(len(data_points)):\n",
    "        cache.append(None)\n",
    "        \n",
    "    for i in tqdm(range(len(data_points))):\n",
    "        # Compute the fidelity\n",
    "        job_dp1 = cache[i]\n",
    "                \n",
    "        if job_dp1 == None:\n",
    "            job_dp1 = qiskit_backend.run(\n",
    "                transpile(\n",
    "                    setup_var_form(data_points[i], params),\n",
    "                    backend=qiskit_backend\n",
    "                )\n",
    "            ).result().get_statevector()\n",
    "            cache[i] = job_dp1\n",
    "                \n",
    "            # Compute fidelities with reference points\n",
    "            data_points[i].fidelity = 0\n",
    "            for ref_index, r in enumerate(reference_points):\n",
    "                # Add the input-set-distance to the cost\n",
    "                    \n",
    "                if (fidelities[ref_index][i] == None):\n",
    "                    fidelities[ref_index][i] = state_fidelity(r, job_dp1)\n",
    "                fidelity_state1 = fidelities[ref_index][i]\n",
    "                    \n",
    "                if (fidelity_state1 > data_points[i].fidelity):\n",
    "                    data_points[i].cluster = ref_index\n",
    "                    data_points[i].fidelity = fidelity_state1\n",
    "                    \n",
    "        # Add the penalty term corresponding to Lagrange multipliers\n",
    "        penalty = 0\n",
    "        for r in range(len(reference_points)):\n",
    "            penalty += fidelities[r][i] - 1\n",
    "        total_cost += penalty ** 2\n",
    "    \n",
    "    # Accumulate the datapoints and update counts for each cluster\n",
    "    for dp in data_points:\n",
    "        centroids[dp.cluster].x += dp.x\n",
    "        centroids[dp.cluster].y += dp.y\n",
    "        centroids[dp.cluster].a += dp.a\n",
    "        centroids[dp.cluster].b += dp.b\n",
    "        counts[dp.cluster] += 1\n",
    "    \n",
    "    # Compute the coordinates of each (euclidean) cluster centroid\n",
    "    for i in range(len(reference_points)):\n",
    "        if (counts[i] > 0):\n",
    "            centroids[i].x = centroids[i].x / counts[i]\n",
    "            centroids[i].y = centroids[i].y / counts[i]\n",
    "            centroids[i].a = centroids[i].a / counts[i]\n",
    "            centroids[i].b = centroids[i].b / counts[i]\n",
    "    \n",
    "    # Calculate the total cost according to the objective function\n",
    "    for i in range(len(data_points)):\n",
    "        centroid_distance = data_points[i].dist(centroids[data_points[i].cluster])\n",
    "        for j in range(len(data_points)):\n",
    "            cost = distance_matrix[i][j] ** alpha\n",
    "            cost += (centroid_distance / max_distance) * lamda\n",
    "            sum_fidels = 0\n",
    "            # The prefactors are constant for each product of fidelities i and j\n",
    "            for r in range(len(reference_points)):\n",
    "                sum_fidels += (1-fidelities[r][i])*(1-fidelities[r][j])\n",
    "            cost *= sum_fidels\n",
    "            total_cost += cost\n",
    "        \n",
    "    print(\"Total cost: \" + str(total_cost))\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now finally ready to perform the experiment! The resulting cluster assignments are plotted once the optimal parameters are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the alpha and lambda parameters of the cost function\n",
    "alpha = 1\n",
    "lamda = 0.725\n",
    "\n",
    "#\n",
    "# Hard-code the three reference states on the bloch-sphere that we will use as references for the clusters\n",
    "#\n",
    "# [1, 0] = |0> and [0, 1] = |1>\n",
    "reference_points = [\n",
    "    Statevector([1, 0, 0, 0]), # 0-0\n",
    "    Statevector([0, 1, 0, 0]), # 0-1\n",
    "    Statevector([0, 0, 1, 0]), # 1-0\n",
    "    #Statevector([0, 0, 0, 1]), # 1-1\n",
    "]\n",
    "\n",
    "# Select the right optimizer\n",
    "optimizer = ADAM(maxiter=13, tol=10^-6, lr=0.1)\n",
    "\n",
    "# Fix the np random seed\n",
    "np.random.seed(12)\n",
    "\n",
    "# Initialize the parameters with random values\n",
    "params_init = np.random.rand(16)\n",
    "print(params_init)\n",
    "\n",
    "# Perform the optimization and store the result\n",
    "optimal_params = optimizer.minimize(fun=objective, x0=params_init).x\n",
    "\n",
    "print(optimal_params)\n",
    "print(\"Total cost: \" + str(objective(optimal_params)))\n",
    "\n",
    "# Divide the points into clusters, according to the information that's stored within the datapoints\n",
    "clusters = []\n",
    "for ref_index in range(len(reference_points)):\n",
    "    current_cluster = []\n",
    "    for dp in data_points:\n",
    "        if ref_index == dp.cluster:\n",
    "            current_cluster.append(dp)\n",
    "    clusters.append(current_cluster)\n",
    "\n",
    "# Plot the clustering of the datapoints projected onto x and y features\n",
    "for c in clusters:\n",
    "    plt.scatter(\n",
    "        [dp.x for dp in c],\n",
    "        [dp.y for dp in c]\n",
    "    )\n",
    "plt.title(\"Clusters after optimizing\")\n",
    "plt.show()\n",
    "\n",
    "for i, c in enumerate(clusters):\n",
    "    print(\"Data Points in cluster \" + str(i) + \":\")\n",
    "    for dp in c:\n",
    "        print(\"[\" + str(dp.x) + \", \" + str(dp.y) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionally print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method determines a permutation of labels such that\n",
    "# the accuracy between the expectation and cluster output\n",
    "# is maximized\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum(w[ind]) * 1.0 / y_pred.size\n",
    "\n",
    "preds = []\n",
    "for dp in data_points:\n",
    "    preds.append(dp.cluster)\n",
    "\n",
    "print(cluster_acc(iris.target, np.array(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
